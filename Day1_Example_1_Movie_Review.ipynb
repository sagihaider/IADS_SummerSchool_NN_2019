{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day1_Example_1_Movie_Review.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagihaider/IADS_SummerSchool_NN_2019/blob/master/Day1_Example_1_Movie_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohoLhVGN3ndJ",
        "colab_type": "text"
      },
      "source": [
        "## Classifying movie reviews: a binary classification example\n",
        "\n",
        "The IMDB dataset\n",
        "\n",
        "You’ll work with the IMDB dataset: \n",
        "\n",
        "* Set of 50,000 highly polarized reviews from the Internet Movie Database. \n",
        "* They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM81enYG4nP1",
        "colab_type": "text"
      },
      "source": [
        "## Question 1: Why use separate training and test sets?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJi6ZFZs49Sq",
        "colab_type": "text"
      },
      "source": [
        "the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into\n",
        "sequences of integers, where each integer stands for a specific word in a dictionary. The following code will load the dataset (when you run it the first time, about 80 MB of data will be downloaded to your machine)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBpCGV8g5BJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading IMDB Dataset\n",
        "\n",
        "from keras.datasets import imdb\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SUQGWGfQgj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "# save np.load\n",
        "np_load_old = np.load\n",
        "\n",
        "# modify the default parameters of np.load\n",
        "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "\n",
        "# call load_data with allow_pickle implicitly set to true\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# restore np.load for future normal usage\n",
        "np.load = np_load_old"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXztc8RlRXYL",
        "colab_type": "text"
      },
      "source": [
        "The variables train_data and test_data are lists of reviews; each review is a list of word indices (encoding a sequence of words). train_labels and test_labels are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ2mBWFUe5_W",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the data\n",
        "\n",
        "You can’t feed lists of integers into a neural network. You have to turn your lists into tensors. There are two ways to do that:\n",
        "\n",
        "1. Pad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, word_indices), and then use as the first layer in your network a layer capable of handling such integer tensors (the Embedding layer).\n",
        "\n",
        "2. One-hot encode your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then you could use as the first layer in your network a Dense layer, capable of handling floating-point vector data. \n",
        "\n",
        "Let’s go with the latter solution to vectorize the data, which you’ll do manually for \n",
        "maximum clarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mrkcXZdeX1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension)) # Creates an all-zero matrix of shape (len(sequences), dimension)\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1. # Sets specific indices of results[i] to 1s\n",
        "    return results\n",
        "  \n",
        "x_train = vectorize_sequences(train_data) # Vectorized training data\n",
        "x_test = vectorize_sequences(test_data) # Vectorized testing data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doJAneJ6gwwy",
        "colab_type": "text"
      },
      "source": [
        "You should also vectorize your labels, which is straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzA6zAH1esN4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1a3ced9-3be9-45e7-8d9c-859dc4636fa2"
      },
      "source": [
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkSBZ-xhhD4O",
        "colab_type": "text"
      },
      "source": [
        "### Building your network\n",
        "\n",
        "* The argument being passed to each `Dense` layer (16) is the number of hidden units of the layer.\n",
        "\n",
        "* Having 16 hidden units means the weight matrix `W` will have shape` (input_dimension, 16)`: the dot product with` W` will project the input data onto a 16-dimensional representation space (and then you’ll add the bias vector `b` and apply the `relu` operation).\n",
        "\n",
        "* There are two key architecture decisions to be made about such a stack of Dense layers:\n",
        "1. How many layers to use\n",
        "2. How many hidden units to choose for each layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEQa0vLDhSzt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "bae69d69-76a0-4d4f-e53c-8d57893d96f7"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0725 14:25:07.122959 140230226061184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0725 14:25:07.153278 140230226061184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0725 14:25:07.162981 140230226061184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 16)                160016    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,305\n",
            "Trainable params: 160,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0oyNCjtnUHZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "38b1f52d-dfb1-440d-d991-999d2e6fd89e"
      },
      "source": [
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "              loss=losses.binary_crossentropy,\n",
        "              metrics=[metrics.binary_accuracy])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0725 14:52:03.155279 140230226061184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0725 14:52:03.164250 140230226061184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0725 14:52:03.173445 140230226061184 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjCpclBEnkj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiHa5UyDnpWt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "8c405e8e-4083-481e-e163-d0ad3b2ace37"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0725 14:52:56.467772 140230226061184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "15000/15000 [==============================] - 4s 286us/step - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.4947\n",
            "Epoch 2/20\n",
            "15000/15000 [==============================] - 2s 141us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947\n",
            "Epoch 3/20\n",
            "15000/15000 [==============================] - 2s 135us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947\n",
            "Epoch 4/20\n",
            "15000/15000 [==============================] - 2s 137us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947\n",
            "Epoch 5/20\n",
            "15000/15000 [==============================] - 2s 136us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947\n",
            "Epoch 6/20\n",
            "15000/15000 [==============================] - 2s 137us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.4947\n",
            "Epoch 7/20\n",
            "15000/15000 [==============================] - 2s 135us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947\n",
            "Epoch 8/20\n",
            "15000/15000 [==============================] - 2s 136us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947\n",
            "Epoch 9/20\n",
            "15000/15000 [==============================] - 2s 136us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.4947\n",
            "Epoch 10/20\n",
            "15000/15000 [==============================] - 2s 136us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.4947\n",
            "Epoch 11/20\n",
            "15000/15000 [==============================] - 2s 136us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.4947\n",
            "Epoch 12/20\n",
            "15000/15000 [==============================] - 2s 134us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.4947\n",
            "Epoch 13/20\n",
            "15000/15000 [==============================] - 2s 136us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.4947\n",
            "Epoch 14/20\n",
            "15000/15000 [==============================] - 2s 134us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947\n",
            "Epoch 15/20\n",
            "15000/15000 [==============================] - 2s 136us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947\n",
            "Epoch 16/20\n",
            "15000/15000 [==============================] - 2s 135us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947\n",
            "Epoch 17/20\n",
            "15000/15000 [==============================] - 2s 134us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947\n",
            "Epoch 18/20\n",
            "15000/15000 [==============================] - 2s 135us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947\n",
            "Epoch 19/20\n",
            "15000/15000 [==============================] - 2s 135us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947\n",
            "Epoch 20/20\n",
            "15000/15000 [==============================] - 2s 138us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfnY9nddo47T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "52cc658a-8f56-4176-def7-e2c3aec1f61f"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "loss='binary_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "results = model.evaluate(x_test, y_test)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "25000/25000 [==============================] - 3s 124us/step - loss: 0.6932 - acc: 0.4991\n",
            "Epoch 2/4\n",
            "25000/25000 [==============================] - 2s 99us/step - loss: 0.6932 - acc: 0.4961\n",
            "Epoch 3/4\n",
            "25000/25000 [==============================] - 2s 96us/step - loss: 0.6932 - acc: 0.4997\n",
            "Epoch 4/4\n",
            "25000/25000 [==============================] - 2s 96us/step - loss: 0.6932 - acc: 0.4959\n",
            "25000/25000 [==============================] - 2s 77us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvaqleazpWnX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9da27818-30f5-4eef-eb5c-e9b0ab86f0ba"
      },
      "source": [
        "results"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.693147878112793, 0.49996]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQrBUrcgh2mz",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}